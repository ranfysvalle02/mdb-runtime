#!/usr/bin/env python3
"""
Semantic Text Splitting and Embedding Example

This example demonstrates the new EmbeddingService that combines:
1. Semantic text splitting (Rust-based, fast and accurate)
2. Embedding generation (via LiteLLM, provider-agnostic)
3. MongoDB storage (structured document format)

The service is configured via manifest.json embedding_config section.
"""

import asyncio
import os
from pathlib import Path

from mdb_runtime import RuntimeEngine
from mdb_runtime.llm import LLMService, EmbeddingService, get_embedding_service


async def main():
    """Main example function"""
    print("üöÄ Semantic Text Splitting and Embedding Example")
    print("=" * 60)
    
    # Get MongoDB connection from environment
    mongo_uri = os.getenv(
        "MONGO_URI", 
        "mongodb://admin:password@mongodb:27017/?authSource=admin"
    )
    db_name = os.getenv("MONGO_DB_NAME", "vector_hacking_db")
    
    # Mask password in logs
    safe_uri = mongo_uri.replace("password", "***") if "password" in mongo_uri else mongo_uri
    print(f"üì° Connecting to MongoDB: {safe_uri}")
    print(f"üì¶ Database: {db_name}\n")
    
    # Initialize the runtime engine
    engine = RuntimeEngine(
        mongo_uri=mongo_uri,
        db_name=db_name
    )
    
    try:
        # Connect to MongoDB
        await engine.initialize()
        print("‚úÖ Engine initialized successfully")
        
        # Load and register the app manifest
        manifest_path = Path("/app/manifest.json")
        if not manifest_path.exists():
            manifest_path = Path(__file__).parent / "manifest.json"
            if not manifest_path.exists():
                print(f"‚ùå Manifest not found. Tried: /app/manifest.json and {manifest_path}")
                return
        
        manifest = await engine.load_manifest(manifest_path)
        success = await engine.register_app(manifest, create_indexes=True)
        
        if not success:
            print("‚ùå Failed to register app")
            return
        
        print(f"‚úÖ App '{manifest['slug']}' registered successfully\n")
        
        # Get embedding config from manifest
        embedding_config = manifest.get("embedding_config", {})
        if not embedding_config.get("enabled", False):
            print("‚ö†Ô∏è  Embedding service not enabled in manifest.json")
            print("   Add 'embedding_config' section with 'enabled: true' to enable it.")
            return
        
        # Get LLM config from manifest
        llm_config = manifest.get("llm_config", {})
        if not llm_config.get("enabled", False):
            print("‚ö†Ô∏è  LLM service not enabled in manifest.json")
            print("   Embedding service requires LLM service to be enabled.")
            return
        
        # Initialize LLM Service
        print("üîß Initializing LLM Service...")
        llm_service = LLMService(config=llm_config)
        print("‚úÖ LLM Service initialized\n")
        
        # Initialize Embedding Service
        print("üîß Initializing Embedding Service...")
        embedding_service = get_embedding_service(llm_service, config=embedding_config)
        print("‚úÖ Embedding Service initialized\n")
        
        # Get a scoped database for the app
        db = engine.get_scoped_db("vector_hacking")
        
        # ============================================
        # Example: Process and Store Text
        # ============================================
        print("üìù Processing and storing text with embeddings...")
        print("-" * 60)
        
        # Sample long text (repeated to ensure multiple chunks)
        long_text = """
        Semantic search relies on vector embeddings to understand the context of data.
        Traditional keyword search looks for exact matches, but vector search looks for conceptual similarity.
        
        Rust is a systems programming language that runs blazingly fast, prevents segfaults, and guarantees thread safety.
        By wrapping a Rust core in Python, we get the developer experience of Python with the performance of Rust.
        
        MongoDB is a NoSQL database that now supports Vector Search via Atlas, allowing you to build
        RAG (Retrieval Augmented Generation) applications directly on your operational data.
        
        The semantic-text-splitter library uses Rust for token counting, ensuring chunks never exceed
        model limits while preserving semantic boundaries. This is crucial for maintaining context
        in RAG applications.
        
        LiteLLM provides a universal wrapper for LLM and embedding APIs, allowing you to switch
        between providers (OpenAI, VoyageAI, Cohere) by simply changing a model string.
        """ * 10  # Repeat to generate multiple chunks
        
        # Process and store
        result = await embedding_service.process_and_store(
            text_content=long_text,
            source_id="doc_101",
            collection=db.voyage_embeddings,
            max_tokens=embedding_config.get("max_tokens_per_chunk", 1000),
            embedding_model=embedding_config.get("default_embedding_model")
        )
        
        print(f"‚úÖ Created {result['chunks_created']} chunks")
        print(f"‚úÖ Inserted {result['documents_inserted']} documents")
        print(f"‚úÖ Source ID: {result['source_id']}\n")
        
        # ============================================
        # Example: Query Stored Embeddings
        # ============================================
        print("üîç Querying stored embeddings...")
        print("-" * 60)
        
        # Find all chunks for this source
        chunks = await db.voyage_embeddings.find(
            {"source_id": "doc_101"}
        ).sort("chunk_index", 1).to_list(length=None)
        
        print(f"‚úÖ Found {len(chunks)} chunks for source 'doc_101'")
        
        for chunk in chunks[:3]:  # Show first 3
            print(f"\n   Chunk {chunk['chunk_index']}:")
            print(f"   Text: {chunk['text'][:100]}...")
            print(f"   Embedding dimensions: {len(chunk['embedding'])}")
            print(f"   Model: {chunk['metadata'].get('model', 'unknown')}")
        
        if len(chunks) > 3:
            print(f"\n   ... and {len(chunks) - 3} more chunks")
        
        print()
        
        # ============================================
        # Example: Process Text Without Storing
        # ============================================
        print("üìù Processing text without storing...")
        print("-" * 60)
        
        sample_text = """
        This is a shorter text that will be chunked and embedded.
        The service will return the chunks with their embeddings,
        but won't store them in the database.
        """ * 5
        
        results = await embedding_service.process_text(
            text_content=sample_text,
            max_tokens=500
        )
        
        print(f"‚úÖ Processed {len(results)} chunks")
        for i, result in enumerate(results):
            print(f"   Chunk {i}: {len(result['text'])} chars, "
                  f"{len(result['embedding'])} dimensions")
        
        print()
        
        # ============================================
        # Summary
        # ============================================
        print("üí° What happened:")
        print("   1. Text was split semantically using Rust-based semantic-text-splitter")
        print("   2. Each chunk was embedded using LiteLLM (VoyageAI in this case)")
        print("   3. Documents were stored in MongoDB with proper structure")
        print("   4. All configuration came from manifest.json")
        print()
        print("‚úÖ Example completed successfully!")
        
    except Exception as e:
        print(f"‚ùå Error: {e}")
        import traceback
        traceback.print_exc()
    
    finally:
        # Cleanup
        await engine.shutdown()
        print("\nüßπ Cleaned up and shut down")


if __name__ == "__main__":
    asyncio.run(main())

